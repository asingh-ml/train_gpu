{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 10807713568318899882\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 2622182079110820315\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_GPU:1\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 4364576654202505950\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 9336564850093370598\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 7966438196\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "    link {\n",
      "      device_id: 1\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "incarnation: 592305377500309145\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080, pci bus id: 0000:03:00.0, compute capability: 6.1\"\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 7967745639\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "    link {\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "incarnation: 10371136554938480517\n",
      "physical_device_desc: \"device: 1, name: GeForce GTX 1080, pci bus id: 0000:04:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0',\n",
       " '/job:localhost/replica:0/task:0/device:GPU:1']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import Input, Bidirectional, LSTM, Embedding, Dense, Dropout, Lambda, Concatenate, CuDNNLSTM\n",
    "from keras.models import Model\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from model.callbacks import F1score\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from model.data_utils import minibatches, pad_sequences\n",
    "from model.crf_layer import ChainCRF\n",
    "from keras import optimizers\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "from model.callbacks import LossHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseKerasModel(object):\n",
    "    \"\"\"Generic class for general methods that are not specific to NER\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"Defines self.config and self.logger\n",
    "        Args:\n",
    "            config: (Config instance) class with hyper parameters,\n",
    "                vocab and embeddings\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.logger = config.logger\n",
    "        self.model = None\n",
    "        self.conf = tf.ConfigProto()\n",
    "        self.conf.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "        self.conf.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "        self.sess = tf.Session(config=self.conf)\n",
    "        set_session(self.sess)\n",
    "        self.saver = None\n",
    "\n",
    "    def batch_iter(self, train, batch_size, return_lengths=False):\n",
    "        \"\"\"\n",
    "        Creates a batch generator for the dataset\n",
    "        :param train: Dataset\n",
    "        :param batch_size: Batch Size\n",
    "        :param return_lengths: If True, generator returns sequence lengths. Used masking data during the evaluation step\n",
    "        :return: (number of batches in dataset, data generator)\n",
    "        \"\"\"\n",
    "        nbatches = (len(train) + batch_size - 1) // batch_size\n",
    "\n",
    "        def data_generator():\n",
    "            while True:\n",
    "                for i, (words, labels) in enumerate(minibatches(train, batch_size)):\n",
    "\n",
    "                    # word_ids = zip(*words)\n",
    "                    word_ids, sequence_lengths = pad_sequences(words, 0, max_length=self.config.max_length)\n",
    "\n",
    "                    if labels:\n",
    "                        labels, _ = pad_sequences(labels, 0, max_length=self.config.max_length)\n",
    "                        labels = [to_categorical(label, num_classes=self.config.ntags) for label in labels] # Change labels to one-hot\n",
    "\n",
    "                    # build dictionary\n",
    "                    inputs = {\n",
    "                        \"word_ids\": np.asarray(word_ids),\n",
    "                    }\n",
    "\n",
    "                    if return_lengths:\n",
    "                        yield(inputs, np.asarray(labels), sequence_lengths)\n",
    "\n",
    "                    else:\n",
    "                        yield (inputs, np.asarray(labels))\n",
    "\n",
    "        return (nbatches, data_generator())\n",
    "\n",
    "    def train(self, train, dev, show_history=False):\n",
    "        batch_size = self.config.batch_size\n",
    "\n",
    "        nbatches_train, train_generator = self.batch_iter(train, batch_size)\n",
    "        nbatches_dev, dev_generator = self.batch_iter(dev, batch_size)\n",
    "\n",
    "\n",
    "        _, f1_generator = self.batch_iter(dev, batch_size, return_lengths=True)\n",
    "        f1 = F1score(f1_generator, nbatches_dev, self.run_evaluate)\n",
    "\n",
    "        callbacks = self.gen_callbacks([f1])\n",
    "\n",
    "        history = self.model.fit_generator(generator=train_generator,\n",
    "                                           steps_per_epoch=nbatches_train,\n",
    "                                           validation_data=dev_generator,\n",
    "                                           validation_steps=nbatches_dev,\n",
    "                                           epochs=self.config.nepochs,\n",
    "                                           callbacks=callbacks) #, nbatches_train\n",
    "\n",
    "        if show_history:\n",
    "            print(history.history['f1'])\n",
    "            pass\n",
    "\n",
    "\n",
    "    def predict_words(self, words_raw):\n",
    "        words = [self.config.processing_word(w) for w in words_raw]\n",
    "        # if type(words[0]) == tuple:\n",
    "        #     words = zip(*words)\n",
    "        # char_ids, word_ids = words\n",
    "\n",
    "        word_ids = np.asarray(words)\n",
    "        s = word_ids.shape\n",
    "        word_ids = word_ids.reshape(-1, s[0])\n",
    "        inputs = [word_ids]\n",
    "\n",
    "        one_hot_preds = self.model.predict_on_batch(inputs)\n",
    "        #print(\"One hot preds: \", one_hot_preds)\n",
    "        one_hot_preds = [a.flatten() for a in one_hot_preds.squeeze()] #Squeeze to remove unnecessary 1st dimension for batch size\n",
    "        #print(\"One hot preds: \", one_hot_preds)\n",
    "\n",
    "        pred_ids = np.argmax(one_hot_preds, axis=1)\n",
    "        #print(\"Pred ids: \", pred_ids)\n",
    "\n",
    "        preds = [self.idx_to_tag[idx] for idx in pred_ids]\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def run_evaluate(self, data_generator, steps_per_epoch):\n",
    "        accs = []\n",
    "        label_true = []\n",
    "        label_pred = []\n",
    "        for i in range(steps_per_epoch):\n",
    "            #try:\n",
    "            x_true, y_true, sequence_lengths = next(data_generator)\n",
    "            y_pred = self.model.predict_on_batch(x_true)\n",
    "\n",
    "            for lab, lab_pred, length in zip(y_true, y_pred,\n",
    "                                             sequence_lengths):\n",
    "                lab = lab[:length]\n",
    "                lab_pred = lab_pred[:length]\n",
    "\n",
    "                lab = np.argmax(lab, axis=1)\n",
    "                lab_pred = np.argmax(lab_pred, axis=1)\n",
    "                accs += [a==b for (a, b) in zip(lab, lab_pred)]\n",
    "\n",
    "\n",
    "                label_true.extend(lab)\n",
    "                label_pred.extend(lab_pred)\n",
    "\n",
    "\n",
    "        label_true = np.asarray(label_true)\n",
    "        #print(\"Truths: \", label_true)\n",
    "        label_pred = np.asarray(label_pred)\n",
    "        #print(\"Preds: \", label_pred)\n",
    "\n",
    "        acc = np.mean(accs)\n",
    "        print(\"acc: \", 100*acc)\n",
    "\n",
    "        micro_score = f1_score(label_true, label_pred, average='micro')\n",
    "        print(' - micro f1: {:04.2f}'.format(micro_score * 100))\n",
    "\n",
    "        macro_score = f1_score(label_true, label_pred, average='macro')\n",
    "        print(' - macro f1: {:04.2f}'.format(macro_score * 100))\n",
    "\n",
    "        weighted_score = f1_score(label_true, label_pred, average='weighted')\n",
    "        print(' - weighted f1: {:04.2f}'.format(weighted_score * 100))\n",
    "\n",
    "        print(classification_report(label_true, label_pred))\n",
    "        return (micro_score, macro_score, weighted_score)\n",
    "\n",
    "    def get_loss(self):\n",
    "        return self._loss\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self.model, name)\n",
    "\n",
    "    def get_optimizer(self):\n",
    "        return self._optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLSTMCRF(BaseKerasModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BLSTMCRF, self).__init__(config)\n",
    "        self._loss = None #losses.sparse_categorical_crossentropy\n",
    "        self._optimizer = optimizers.Nadam(lr=self.config.lr)\n",
    "        self.idx_to_tag = {idx: tag for tag, idx in\n",
    "                           self.config.vocab_tags.items()}\n",
    "\n",
    "    def build(self):\n",
    "        inputs = [] #Create input for Model\n",
    "\n",
    "        # build word embeddings\n",
    "        input_words = Input(shape=(None,), dtype='int32', name='word_ids')\n",
    "        inputs.append(input_words)\n",
    "        if self.config.embeddings is None:\n",
    "            word_embeddings = Embedding(input_dim=self.config.nwords,\n",
    "                                        output_dim=self.config.dim_word,\n",
    "                                        mask_zero=True,\n",
    "                                        name=\"word_embeddings\")(input_words)\n",
    "        else:\n",
    "            word_embeddings = Embedding(input_dim=self.config.nwords,\n",
    "                                        output_dim=self.config.dim_word,\n",
    "                                        mask_zero=False,\n",
    "                                        weights=[self.config.embeddings],\n",
    "                                        trainable=self.config.train_embeddings,\n",
    "                                        name=\"word_embeddings\")(input_words)\n",
    "\n",
    "        word_embeddings = Dropout(self.config.dropout)(word_embeddings)\n",
    "        encoded_text = Bidirectional(CuDNNLSTM(units=self.config.hidden_size_lstm, return_sequences=True),\n",
    "                                     name=\"bidirectional_1\")(word_embeddings)\n",
    "        encoded_text = Dropout(self.config.dropout)(encoded_text)\n",
    "        encoded_text = Bidirectional(CuDNNLSTM(units=self.config.hidden_size_lstm, return_sequences=True),\n",
    "                                     name=\"bidirectional_2\")(encoded_text)\n",
    "        encoded_text = Dropout(self.config.dropout)(encoded_text)\n",
    "        encoded_text = TimeDistributed(Dense(self.config.ntags))(encoded_text)\n",
    "        encoded_text = Dropout(self.config.dropout)(encoded_text)\n",
    "        #encoded_text = Dense(100, activation='tanh')(encoded_text)\n",
    "\n",
    "        if self.config.use_crf:\n",
    "#             crf = CRF(self.config.ntags, sparse_target=False)\n",
    "            crf = ChainCRF()\n",
    "            self._loss = crf.loss\n",
    "#             self._loss = crf.loss_function\n",
    "            pred = crf(encoded_text)\n",
    "\n",
    "        else:\n",
    "            self._loss = 'categorical_crossentropy'\n",
    "            pred = Dense(self.config.ntags, activation='softmax')(encoded_text)\n",
    "\n",
    "        self.model = Model(inputs, pred)\n",
    "\n",
    "\n",
    "    def gen_callbacks(self, callbacks_list):\n",
    "        lrate = LearningRateScheduler(self.step_decay)\n",
    "        callbacks_list.append(lrate)\n",
    "\n",
    "        #loss_history = LossHistory(self.step_decay)\n",
    "        #callbacks_list.append(loss_history)\n",
    "        return callbacks_list\n",
    "\n",
    "\n",
    "    def step_decay(self, epoch):\n",
    "        initial_lrate = self.config.lr\n",
    "        decay = self.config.lr_decay\n",
    "        epochs_drop = self.config.epoch_drop\n",
    "        lrate = initial_lrate * math.pow(decay,\n",
    "                                         math.floor((1+epoch)/epochs_drop))\n",
    "        return lrate\n",
    "\n",
    "    def plot_history(self, history):\n",
    "        plt.plot(history.losses)\n",
    "        plt.title('model losses')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.data_utils import Dataset\n",
    "from model.config import Config\n",
    "from keras.models import load_model\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:2403: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:2403: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "word_ids (InputLayer)        (None, None)              0         \n",
      "_________________________________________________________________\n",
      "word_embeddings (Embedding)  (None, None, 50)          5512550   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, None, 300)         242400    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, None, 300)         0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, None, 300)         542400    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, None, 300)         0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, None, 3)           903       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, None, 3)           0         \n",
      "_________________________________________________________________\n",
      "chain_crf_1 (ChainCRF)       (None, None, 3)           15        \n",
      "=================================================================\n",
      "Total params: 6,298,268\n",
      "Trainable params: 785,718\n",
      "Non-trainable params: 5,512,550\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "model = BLSTMCRF(config)\n",
    "model.build()\n",
    "model.compile(optimizer=model.get_optimizer(), loss=model.get_loss())  # , metrics=['categorical_accuracy']\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets\n",
    "dev = Dataset(config.filename_dev, config.processing_word,\n",
    "              config.processing_tag, config.max_iter)\n",
    "train = Dataset(config.filename_train, config.processing_word,\n",
    "                config.processing_tag, config.max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "23170/23170 [==============================] - 2793s 121ms/step - loss: 3.6950 - val_loss: 4.2269\n",
      "acc:  93.60303105531511\n",
      " - micro f1: 93.60\n",
      " - macro f1: 75.31\n",
      " - weighted f1: 92.85\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.35      0.49   1161349\n",
      "           1       0.94      0.99      0.96  21617464\n",
      "           2       0.93      0.72      0.81   2171441\n",
      "\n",
      "    accuracy                           0.94  24950254\n",
      "   macro avg       0.88      0.69      0.75  24950254\n",
      "weighted avg       0.93      0.94      0.93  24950254\n",
      "\n",
      "Epoch 2/8\n",
      "23170/23170 [==============================] - 2792s 120ms/step - loss: 3.1016 - val_loss: 4.1099\n",
      "acc:  94.35621777638015\n",
      " - micro f1: 94.36\n",
      " - macro f1: 77.68\n",
      " - weighted f1: 93.70\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.37      0.51   1161349\n",
      "           1       0.95      0.99      0.97  21617464\n",
      "           2       0.94      0.78      0.86   2171441\n",
      "\n",
      "    accuracy                           0.94  24950254\n",
      "   macro avg       0.90      0.72      0.78  24950254\n",
      "weighted avg       0.94      0.94      0.94  24950254\n",
      "\n",
      "Epoch 3/8\n",
      "23170/23170 [==============================] - 2791s 120ms/step - loss: 3.0198 - val_loss: 4.0512\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.36      0.51   1161349\n",
      "           1       0.95      0.99      0.97  21617464\n",
      "           2       0.94      0.82      0.87   2171441\n",
      "\n",
      "    accuracy                           0.95  24950254\n",
      "   macro avg       0.91      0.72      0.78  24950254\n",
      "weighted avg       0.94      0.95      0.94  24950254\n",
      "\n",
      "Epoch 4/8\n",
      "23170/23170 [==============================] - 2791s 120ms/step - loss: 2.9718 - val_loss: 4.0254\n",
      "acc:  94.87254117733632\n",
      " - micro f1: 94.87\n",
      " - macro f1: 79.48\n",
      " - weighted f1: 94.29\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.39      0.53   1161349\n",
      "           1       0.95      0.99      0.97  21617464\n",
      "           2       0.94      0.82      0.88   2171441\n",
      "\n",
      "    accuracy                           0.95  24950254\n",
      "   macro avg       0.91      0.74      0.79  24950254\n",
      "weighted avg       0.95      0.95      0.94  24950254\n",
      "\n",
      "Epoch 5/8\n",
      "23169/23170 [============================>.] - ETA: 0s - loss: 2.9396"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "model.train(train, dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ast import literal_eval\n",
    "import pandas as pd\n",
    "import re\n",
    "from seqeval.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for storing model object\n",
    "if not os.path.exists('./results/saves'):\n",
    "    os.makedirs('./results/saves')\n",
    "\n",
    "# Save model\n",
    "model.save_weights('./results/saves/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Trained and saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.BLSTMCRF at 0x7f7dd135e080>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
